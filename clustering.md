# 聚类算法介绍:

聚类分析又称群分析，它是研究（样品或指标）分类问题的一种统计分析方法，同时也是数据挖掘的一个重要算法。

聚类（Cluster）分析是由若干模式（Pattern）组成的，通常，模式是一个度量（Measurement）的向量，或者是多维空间中的一个点。

聚类分析以相似性为基础，在一个聚类中的模式之间比不在同一聚类中的模式之间具有更多的相似性。

目前在许多领域都得到了广泛的研究和成功的应用，如用于模式识别、数据分析、图像处理、市场研究、客户分割、Web文档分类等。

聚类就是按照某个特定标准(如距离准则)把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。

即聚类后同一类的数据尽可能聚集到一起，不同数据尽量分离。

### k-means聚类算法

k-means是划分方法中较经典的聚类算法之一。

由于该算法的效率高，所以在对大规模数据进行聚类时被广泛应用。

目前，许多算法均围绕着该算法进行扩展和改进。
　
k-means算法以k为参数，把n个对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。

k-means算法的处理过程如下：首先，随机地 选择k个对象，每个对象初始地代表了一个簇的平均值或中心;对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇;然后重新计算每个簇的平均值。 

这个过程不断重复，直到准则函数收敛。通常，采用平方误差准则，其定义如下：这里E是数据库中所有对象的平方误差的总和，p是空间中的点，mi是簇Ci的平均值。

该目标函数使生成的簇尽可能紧凑独立，使用的距离度量是欧几里得距离,当然也可以用其他距离度量。k-means聚类算法的算法流程如下：
    
    输入：包含n个对象的数据库和簇的数目k；
    
    输出：k个簇，使平方误差准则最小。
    
  ***步骤：***
  ```
　　(1) 任意选择k个对象作为初始的簇中心；
　　(2) repeat；
　　(3) 根据簇中对象的平均值，将每个对象(重新)赋予最类似的簇；
　　(4) 更新簇的平均值，即计算每个簇中对象的平均值；
　　(5) until不再发生变化。
```

K均值聚类 

**何时使用？**

>当你事先知道你将找到多少个分组的时候。（这个就比较尴尬了，因为很多情况下，我们并不知道要聚多少个类）

***工作方式***

> 该算法可以随机将每个观察（observation）分配到 k 类中的一类，然后计算每个类的平均。
接下来，它重新将每个观察分配到与其最接近的均值的类别，然后再重新计算其均值。
这一步不断重复，直到不再需要新的分配为止。

***有效案例***
```
假设有一组 9 位足球运动员，他们中每个人都在这一赛季进了一定数量的球（假设在 3-30 之间）。

然后我们要将他们分成几组——比如 3 组。

第一步：需要我们将这些运动员随机分成 3 组并计算每一组的均值。

    第 1 组
        
        运动员 A（5 个球）、运动员 B（20 个球）、运动员 C（11 个球）
        该组平均=(5 + 20 + 11) / 3 = 12
    
    第 2 组
        
        运动员 D（5 个球）、运动员 E（9 个球）、运动员 F（19 个球）
        该组平均=11
    
    第 3 组
        
        运动员 G（30 个球）、运动员 H（3 个球）、运动员 I（15 个球）
        该组平均=16
        
第二步：对于每一位运动员，将他们重新分配到与他们的分数最接近的均值的那一组；
比如，运动员 A（5 个球）被重新分配到第 2 组（均值=11）。

然后再计算新的均值。

    第 1 组（原来的均值=12）
    
        运动员 C（11 个球）、运动员 E（9 个球）
        新的平均=(11 + 9) / 2 = 10
    
    第 2 组（原来的均值=11）
        运动员 A（5 个球）、运动员 D（5 个球）、运动员 H（3 个球）
        新的平均=4.33
    
    第 3 组（原来的均值=16）
    
        运动员 B（20 个球）、运动员 F（19 个球）、运动员 G（30 个球）、运动员 I（15 个球）
        新的平均=21
        
        
不断重复第二步，直到每一组的均值不再变化。对于这个简单的任务，下一次迭代就能达到我们的目标。

现在就完成了，你已经从原数据集得到了 3 个聚类！

    第 1 组（原来的均值=10）
    
        运动员 C（11 个球）、运动员 E（9 个球）、运动员 I（15 个球）
        最终平均=11.3
        
    第 2 组（原来的均值=4.33）
    
        运动员 A（5 个球）、运动员 D（5 个球）、运动员 H（3 个球）
        最终平均=4.33
        
    第 3 组（原来的均值=21）
    
        运动员 B（20 个球）、运动员 F（19 个球）、运动员 G（30 个球）、
        最终平均=23
```
通过这个例子，该聚类可能能够对应这些运动员在球场上的位置——比如防守、中场和进攻。

K-均值在这里有效，是因为我们可以合理地预测这些数据会自然地落到这三个分组中。

以这种方式，当给定一系列表现统计的数据时，机器就能很好地估计任何足球队的队员的位置——可用于体育分析，也能用于任何将数据集分类为预定义分组的其它目的的分类任务。
（K均值作为用的最广泛的聚类算法，例子太多了，而且我觉得原文的例子举的也比较一般。）

更加细微的细节

上面所描述的算法还有一些变体。最初的「种子」聚类可以通过多种方式完成。这里，我们随机将每位运动员分成了一组，然后计算该组的均值。这会导致最初的均值可能会彼此接近，这会增加后面的步骤。（随机选初始点会增加计算量）

另一种选择种子聚类的方法是每组仅一位运动员，然后开始将其他运动员分配到与其最接近的组。这样返回的聚类是更敏感的初始种子，从而减少了高度变化的数据集中的重复性。但是，这种方法有可能减少完成该算法所需的迭代次数，因为这些分组实现收敛的时间会变得更少。（这不还是K均值吗。。）

K-均值聚类的一个明显限制是你必须事先提供预期聚类数量的假设。目前也存在一些用于评估特定聚类的拟合的方法。比如说，聚类内平方和（Within-Cluster Sum-of-Squares）可以测量每个聚类内的方差。聚类越好，整体 WCSS 就越低。（讨论如何选K值又是一个大话题，而且衡量聚类好坏的标准不止WCSS一种。后面我找个时间总结一下，补充在这里）